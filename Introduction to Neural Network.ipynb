{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "\n",
    "# Data-X: Introduction to Neural Network\n",
    "\n",
    "**Author:** Alexander Fred-Ojala\n",
    "\n",
    "**Sources:** Francois Chollet, Sebastian Raschka, Aurélien Géron, etc.\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Neural Network with TensorFlow and Keras\n",
    "\n",
    "These tutorials use `tf.keras`, TensorFlow's high-level Python API for building and training deep learning models. To learn more about using Keras with TensorFlow, see the TensorFlow Keras Guide.\n",
    "\n",
    "TensorFlow 2.x has integrated Keras and all it's functionality. Import all functionality from `tf.keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Keras\n",
    "Modular, powerful and intuitive Deep Learning python library built on TensorFlow, CNTK, Theano.\n",
    "* Minimalist, user-friendly interface\n",
    "* Modular\n",
    "* Deep integration with Tensorflow (`tf.keras`)\n",
    "* Works on CPUs and GPUs\n",
    "* Open-source, developed and maintained by a community of contributors, and\n",
    "publicly hosted on github\n",
    "* Extremely well documented, lots of working examples: https://keras.io/\n",
    "* Very shallow learning curve —> it is by far one of the best tools for experimenting, both for beginners and experts\n",
    "* Easy to extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras \"Hello World\" on Iris\n",
    "\n",
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== =====\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = datasets.load_iris()\n",
    "\n",
    "print(data.DESCR[:980])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data['data']\n",
    "y = data['target']\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode y\n",
    "import pandas as pd\n",
    "\n",
    "y = pd.get_dummies(y).values\n",
    "y[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(90, 3)\n",
      "(60, 4)\n"
     ]
    }
   ],
   "source": [
    "# train test split, plus randomize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, test_size=0.4,\n",
    "                                                    random_state=1337,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sequential model\n",
    "The simplest model in Keras is the Sequential model, a linear stack of layers. In Keras, you assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers: the tf.keras.Sequential model.\n",
    "\n",
    "To build a simple, fully-connected network (i.e. multi-layer perceptron):\n",
    "\n",
    "* **Sequential model** Allows us to build NNs like legos, by adding one layer on top of the other, and swapping layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data structure in Keras is a model\n",
    "# The model is an object in which we organize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential() # instantiate empty Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import layer classes and stack layers (in an NN model for example), by using `.add()`\n",
    "\n",
    "# Specifying the input shape\n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a  Sequential model needs to receive information about its input shape.\n",
    "\n",
    "**The following snippets are strictly equivalent:**\n",
    "> * `model.add(Dense(32, input_shape=(784,)))`\n",
    "> * `model.add(Dense(32, input_dim=784))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model contruction (architecture build computational graph)\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add( Dense(units=64, activation='relu', \\\n",
    "                 input_shape=(4,) ))\n",
    "\n",
    "model.add( Dense(units=3, activation='softmax') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation phase, specify learning process\n",
    "\n",
    "Run `.compile()` on the model to specify learning process.\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the  compile method. It receives three arguments:\n",
    "\n",
    "* **A loss function:** This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `categorical_crossentropy` or `mse`), or it can be an objective function.\n",
    "* **An optimizer:** This could be the string identifier of an existing optimizer (such as `rmsprop`, `gradientdescent`, or `adam`), or an instance of the Optimizer class.\n",
    "* **(Optional) A list of metrics:** For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also specify our own optimizer or loss function (even build it ourselves)\n",
    "\n",
    "```python\n",
    "# or with we can specify loss function or optimizer\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = SGD(lr=0.001, momentum = 0.9, nesterov=True),\n",
    "             metrics = ['accuracy'])\n",
    "```\n",
    "\n",
    "### Different optimizers and their trade-offs\n",
    "To read more about gradient descent optimizers, hyperparameters etc. This is a recommended reading: http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples\n",
      "Epoch 1/50\n",
      "90/90 [==============================] - 1s 8ms/sample - loss: 1.2440 - accuracy: 0.3556\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - 0s 67us/sample - loss: 1.1639 - accuracy: 0.3667\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - 0s 69us/sample - loss: 1.0991 - accuracy: 0.4778\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - 0s 84us/sample - loss: 1.0333 - accuracy: 0.5556\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - 0s 73us/sample - loss: 0.9772 - accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - 0s 92us/sample - loss: 0.9198 - accuracy: 0.6111\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - 0s 77us/sample - loss: 0.8736 - accuracy: 0.6333\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - 0s 78us/sample - loss: 0.8430 - accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - 0s 86us/sample - loss: 0.8017 - accuracy: 0.5778\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - 0s 91us/sample - loss: 0.7849 - accuracy: 0.5778\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - 0s 80us/sample - loss: 0.7623 - accuracy: 0.6000\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - 0s 80us/sample - loss: 0.7460 - accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - 0s 62us/sample - loss: 0.7319 - accuracy: 0.8333\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - 0s 79us/sample - loss: 0.7185 - accuracy: 0.8333\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - 0s 76us/sample - loss: 0.7046 - accuracy: 0.8222\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - 0s 85us/sample - loss: 0.6898 - accuracy: 0.8111\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - 0s 86us/sample - loss: 0.6758 - accuracy: 0.8333\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - 0s 75us/sample - loss: 0.6620 - accuracy: 0.8556\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - 0s 80us/sample - loss: 0.6488 - accuracy: 0.8778\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - 0s 90us/sample - loss: 0.6364 - accuracy: 0.9111\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - 0s 108us/sample - loss: 0.6273 - accuracy: 0.9333\n",
      "Epoch 22/50\n",
      "90/90 [==============================] - 0s 71us/sample - loss: 0.6139 - accuracy: 0.9333\n",
      "Epoch 23/50\n",
      "90/90 [==============================] - 0s 110us/sample - loss: 0.6027 - accuracy: 0.9333\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - 0s 110us/sample - loss: 0.5923 - accuracy: 0.9222\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - 0s 92us/sample - loss: 0.5829 - accuracy: 0.9222\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - 0s 78us/sample - loss: 0.5742 - accuracy: 0.9222\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - 0s 97us/sample - loss: 0.5659 - accuracy: 0.9111\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - 0s 79us/sample - loss: 0.5568 - accuracy: 0.9222\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - 0s 170us/sample - loss: 0.5485 - accuracy: 0.9222\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - 0s 78us/sample - loss: 0.5400 - accuracy: 0.9444\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - 0s 64us/sample - loss: 0.5327 - accuracy: 0.9444\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - 0s 76us/sample - loss: 0.5257 - accuracy: 0.9444\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - 0s 156us/sample - loss: 0.5178 - accuracy: 0.9444\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - 0s 98us/sample - loss: 0.5118 - accuracy: 0.9333\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - 0s 85us/sample - loss: 0.5047 - accuracy: 0.9444\n",
      "Epoch 36/50\n",
      "90/90 [==============================] - 0s 85us/sample - loss: 0.4983 - accuracy: 0.9444\n",
      "Epoch 37/50\n",
      "90/90 [==============================] - 0s 74us/sample - loss: 0.4919 - accuracy: 0.9556\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - 0s 100us/sample - loss: 0.4855 - accuracy: 0.9556\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - 0s 170us/sample - loss: 0.4796 - accuracy: 0.9444\n",
      "Epoch 40/50\n",
      "90/90 [==============================] - 0s 75us/sample - loss: 0.4745 - accuracy: 0.9444\n",
      "Epoch 41/50\n",
      "90/90 [==============================] - 0s 103us/sample - loss: 0.4682 - accuracy: 0.9444\n",
      "Epoch 42/50\n",
      "90/90 [==============================] - 0s 79us/sample - loss: 0.4640 - accuracy: 0.9444\n",
      "Epoch 43/50\n",
      "90/90 [==============================] - 0s 109us/sample - loss: 0.4576 - accuracy: 0.9556\n",
      "Epoch 44/50\n",
      "90/90 [==============================] - 0s 108us/sample - loss: 0.4526 - accuracy: 0.9556\n",
      "Epoch 45/50\n",
      "90/90 [==============================] - 0s 68us/sample - loss: 0.4475 - accuracy: 0.9556\n",
      "Epoch 46/50\n",
      "90/90 [==============================] - 0s 69us/sample - loss: 0.4433 - accuracy: 0.9556\n",
      "Epoch 47/50\n",
      "90/90 [==============================] - 0s 131us/sample - loss: 0.4381 - accuracy: 0.9444\n",
      "Epoch 48/50\n",
      "90/90 [==============================] - 0s 87us/sample - loss: 0.4330 - accuracy: 0.9444\n",
      "Epoch 49/50\n",
      "90/90 [==============================] - 0s 261us/sample - loss: 0.4287 - accuracy: 0.9556\n",
      "Epoch 50/50\n",
      "90/90 [==============================] - 0s 128us/sample - loss: 0.4250 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a33529590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model by iterating over the training data in batches\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 50, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98333335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Evaluate the model Accuracy on test set\n",
    "model.evaluate(X_test, y_test,verbose=False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on new data:\n",
    "\n",
    "class_probabilities = model.predict(X_test)\n",
    "\n",
    "# gives output of the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0735921 , 0.57182676, 0.3545812 ],\n",
       "       [0.8469639 , 0.14558475, 0.00745141],\n",
       "       [0.0100765 , 0.41334435, 0.5765792 ],\n",
       "       [0.78902245, 0.19809067, 0.01288692],\n",
       "       [0.00633049, 0.36085504, 0.6328145 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probabilities[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Intro to NN in TensorFlow\n",
    "\n",
    "Example taken from Google Docs\n",
    "\n",
    "We are now going to recognize hand-written digits.\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST.png](https://www.tensorflow.org/images/MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the most classic NN dataset\n",
    "The MNIST data is split into three parts: 60,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test).\n",
    "\n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST-Matrix.png](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space, with a very rich structure (warning: computationally intensive visualizations).\n",
    "\n",
    "Flattening the data throws away information about the 2D structure of the image. Isn't that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression (defined below), won't.\n",
    "\n",
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [60000, 784]. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "![https://www.tensorflow.org/images/mnist-train-xs.png](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "\n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras. \\\n",
    "                            datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: (60000, 28, 28)\n",
      "Test input shape: (10000, 28, 28)\n",
      "Input data type: uint8\n"
     ]
    }
   ],
   "source": [
    "# input information\n",
    "print('Train input shape:',x_train.shape)\n",
    "print('Test input shape:',x_test.shape)\n",
    "print('Input data type:',x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-max values: 0 255\n"
     ]
    }
   ],
   "source": [
    "print('Min-max values:',np.min(x_train),np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Image:')\n",
    "plt.imshow(x_train[1],cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output shape: (60000,)\n",
      "Test output shape: (10000,)\n",
      "Data type: uint8\n"
     ]
    }
   ],
   "source": [
    "# Output information:\n",
    "print('Train output shape:',y_train.shape)\n",
    "print('Test output shape:',y_test.shape)\n",
    "print('Data type:',y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "First 10 outputs:\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print('Unique labels:',np.unique(y_train))\n",
    "print('First 10 outputs:')\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, flatten inputs, and convert datatype\n",
    "x_train = x_train.reshape(60000, 784). \\\n",
    "                    astype('float32') / 255\n",
    "\n",
    "x_test = x_test.reshape(10000, 784) \\\n",
    "                    .astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load standard NN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model constructor\n",
    "model = Sequential()\n",
    "# Add layers sequentially\n",
    "model.add(Dense(300, activation=tf.nn.relu, \\\n",
    "                    input_shape=(784,)))\n",
    "\n",
    "# Second..\n",
    "model.add(Dense(200, activation=tf.nn.relu))\n",
    "\n",
    "# Third..\n",
    "model.add(Dense(100, activation=tf.nn.relu))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 4s 90us/sample - loss: 0.2497 - accuracy: 0.9233 - val_loss: 0.1186 - val_accuracy: 0.9647\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 4s 81us/sample - loss: 0.1011 - accuracy: 0.9699 - val_loss: 0.0951 - val_accuracy: 0.9739\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 4s 85us/sample - loss: 0.0690 - accuracy: 0.9790 - val_loss: 0.1002 - val_accuracy: 0.9732\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 4s 86us/sample - loss: 0.0518 - accuracy: 0.9844 - val_loss: 0.1208 - val_accuracy: 0.9707\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 4s 83us/sample - loss: 0.0415 - accuracy: 0.9879 - val_loss: 0.0979 - val_accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "NO_EPOCHS = 5\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=NO_EPOCHS,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9791\n"
     ]
    }
   ],
   "source": [
    "test_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy\n",
    "plt.plot(range(NO_EPOCHS),history.history['val_accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place tensors on the CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(x, y):\n",
    "    #normalize and expand\n",
    "    x = tf.cast(x, tf.float32)/255.\n",
    "    x = tf.expand_dims(x, -1)\n",
    "\n",
    "    #cast the labels\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(x, y):  \n",
    "    #convert to tensors and shuffle\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x , y)).shuffle(len(x)-1)\n",
    "\n",
    "    #extract batches\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    #preprocess the batch\n",
    "    dataset = dataset.map(pre_process, num_parallel_calls = 4)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data (flatten input, convert to floating point and normalize)\n",
    "# Then convert to Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=60000)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches\n",
    "dataset = dataset.batch(64)\n",
    "dataset # creates a new dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Vanilla) ANN Network structure\n",
    "* Any Neural Network with one hidden layer can be a Universal Function Approximator. Source: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "* The number of input nodes are equal to the number of features\n",
    "* The number of output nodes are equal to the number of classes (for classification tasks)\n",
    "* A bias term is added to every layer that only feeds in a 1, that adds an extra degree of freedom for every functional input value to the next function\n",
    "\n",
    "# How deep should we go?\n",
    "* We can overfit Neural Nets, one way to combat that is by using dropout and regularization\n",
    "* Predictions will usually be better when we increase depth of network and widen it (increase the number of neurons in every layer)\n",
    "\n",
    "# Activation Functions\n",
    "* Classically the sigmoid function was used in the hidden layers (simplest function between 0 - 1). Logit function.\n",
    "* Nowadays it is more common to use the ReLU (Rectified Linear Unit). Much quicker! For deep networks sigmoid might not want to converge at all. Much better to handle exploding and vanishing gradients (Leaky Relu). Can also combat that with *Batch Normalization*.\n",
    "* For the input layer we send in the (standardized) values.\n",
    "* For the output layer we often use a softmax function (multi-class classification) or a sigmoid function (binary classification). Softmax only works if the classes are mutually exclusive, i.e. we only try to label one pattern in every training example.\n",
    "\n",
    "\n",
    "# Training algorithm steps\n",
    "* Train a model to make a prediction\n",
    "* Compute distance between predictions and true values\n",
    "* Modify weights and biases to lower error\n",
    "\n",
    "\n",
    "# Overfitting\n",
    "* Mostly because our network has too many degrees of freedom (neurons in the network)\n",
    "* Can use L1 and L2 regularization on the cost function\n",
    "* Drop out (used to mitigate the effects of too many degrees of freedom)\n",
    "\n",
    "# ANNs are not great at classifying images\n",
    "* We don't make use of the image shapes and curves. Shape info is lost when we flatten arrays.\n",
    "\n",
    "# ANN One Layer Softmax Classification\n",
    "\n",
    "What we will accomplish in this section:\n",
    "\n",
    "- Create a softmax regression function that is a model for recognizing MNIST digits, based on looking at every pixel in the image\n",
    "- Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples (and run our first Tensorflow session to do so)\n",
    "- Check the model's accuracy with our test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced tf2 model training\n",
    "\n",
    "Work in progress (porting to tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and input size\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neuron layers (ReLU in hidden layers)\n",
    "# We'll take care of Softmax for output with loss function\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    # X input to neuron\n",
    "    # number of neurons for the layer\n",
    "    # name of layer\n",
    "    # pass in eventual activation function\n",
    "    n_inputs = int(X.shape[1])\n",
    "\n",
    "    # initialize weights to prevent vanishing / exploding gradients\n",
    "    stddev = 2 / np.sqrt(n_inputs)\n",
    "    init = tf.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "    # Initialize weights for the layer\n",
    "    W = tf.Variable(init((n_inputs, n_neurons)), name=\"weights\")\n",
    "    # biases\n",
    "    b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "\n",
    "    # Output from every neuron\n",
    "    Z = tf.matmul(X, W) + b\n",
    "    if activation is not None:\n",
    "        return activation(Z)\n",
    "    else:\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = neuron_layer(x_train, n_hidden1, name=\"hidden1\",\n",
    "                       activation=tf.nn.relu)\n",
    "hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                       activation=tf.nn.relu)\n",
    "logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step with Gradient Descent\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (that also optimizes Softmax for output):\n",
    "\n",
    "\n",
    "# logits are from the last output of the dnn\n",
    "xentropy = tf.keras.losses.sparse_categorical_crossentropy(y_train, logits, from_logits=True)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(x, label):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(x)\n",
    "#         loss = loss_object(label, predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "# \n",
    "#     train_loss(loss)\n",
    "#     train_accuracy(label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
